
<link rel="stylesheet" href="style.css">

<script defer data-url="https://devhunt.org/tool/lynkr" src="https://cdn.jsdelivr.net/gh/sidiDev/devhunt-banner/indexV0.js"></script>


# Lynkr â€“ Claude Code-Compatible Proxy for Databricks 
#### Lynkr is an open-source Claude Code-compatible proxy that allows the Claude Code CLI to run directly with any LLMs without losing the features offered by Anthropic backend. It supports MCP servers, Git workflows, repo intelligence, workspace tools,  prompt caching for LLM-powered development and many other features.
<!--
SEO Keywords:
Databricks, Claude Code, Anthropic, Azure Anthropic,
LLM tools, LLM agents, Model Context Protocol, MCP,
developer tools, proxy, git automation, AI developer tools,
prompt caching, Node.js
-->


---

#  Lynkr  
**MCP â€¢ Git Tools â€¢ Repo Intelligence â€¢ Prompt Caching â€¢ Workspace Automation**

[â­ Star on GitHub](https://github.com/vishalveerareddy123/Lynkr) Â·  
[ğŸ“˜ Documentation](https://deepwiki.com/vishalveerareddy123/Lynkr) Â·  
[ğŸ™ Source Code](https://github.com/vishalveerareddy123/Lynkr)

---

# ğŸš€ What is Lynkr?

**Lynkr** is an open-source **Claude Code-compatible backend proxy** that lets you run the **Claude Code CLI** and Claude-style tools **directly against Databricks** or **Azure-hosted Anthropic models** instead of the default Anthropic cloud.

It enables full repo-aware LLM workflows:

- code navigation  
- diff review  
- Git operations  
- test execution  
- workspace tools  
- Model Context Protocol (MCP) servers  
- repo indexing and project intelligence  
- prompt caching  
- conversational sessions  

This makes Databricks a first-class environment for **AI-assisted software development**, **LLM agents**, **automated refactoring**, **debugging**, and **ML/ETL workflow exploration**.

---

# ğŸŒŸ Key Features (SEO Summary)

### âœ” Claude Code-compatible API (`/v1/messages`)  
Emulates Anthropicâ€™s backend so the **Claude Code CLI works without modification**.

### âœ” Works with Databricks LLM Serving  
Supports **Databricks-hosted Claude Sonnet / Haiku models**, or any LLM served from Databricks.

### âœ” Supports Azure Anthropic models
Route Claude Code requests into Azure's `/anthropic/v1/messages` endpoint.

### âœ” Supports Azure OpenAI models
Connect to Azure OpenAI deployments (GPT-4o, etc.) with full tool calling support.

### âœ” Supports OpenRouter (100+ models)
Access GPT-4o, Claude, Gemini, Llama, and more through a single unified API with full tool calling support.

### âœ” Supports llama.cpp (Local GGUF Models)
Run any GGUF model locally with maximum performance using llama.cpp's optimized C++ inference engine.

### âœ” Full Model Context Protocol (MCP) integration  
Auto-discovers MCP manifests and exposes them as tools for smart workflows.

### âœ” Repo Intelligence: `CLAUDE.md`, Symbol Index, Cross-file analysis  
Lynkr builds a repo index using SQLite + Tree-sitter for rich context.

### âœ” Git Tools and Workflow Automation  
Commit, push, diff, stage, generate release notes, etc.

### âœ” Prompt Caching (LRU + TTL)  
Reuses identical prompts to reduce cost + latency.

### âœ” Workspace Tools
Task tracker, file I/O, test runner, index rebuild, etc.

### âœ” Client-Side Tool Execution (Passthrough Mode)
Tools can execute on the Claude Code CLI side instead of the server, enabling local file operations and commands.

### âœ” Titans-Inspired Long-Term Memory System
Automatic extraction and retrieval of conversation memories using surprise-based filtering, FTS5 semantic search, and multi-signal ranking.

### âœ” Fully extensible Node.js architecture
Add custom tools, policies, or backend adapters.

---

# ğŸ“š Table of Contents

- [What Lynkr Solves](#-what-lynkr-solves)
- [Architecture Overview](#-architecture-overview)
- [Installation](#-installation)
- [Configuring Providers (Databricks & Azure Anthropic)](#-configuring-providers)
- [Using Lynkr With Claude Code CLI](#-using-lynkr-with-claude-code-cli)
- [Repo Intelligence & Indexing](#-repo-intelligence--indexing)
- [Long-Term Memory System (Titans-Inspired)](#-long-term-memory-system-titans-inspired)
- [Prompt Caching](#-prompt-caching)
- [MCP (Model Context Protocol) Integration](#-model-context-protocol-mcp)
- [Git Tools](#-git-tools)
- [Client-Side Tool Execution (Passthrough Mode)](#-client-side-tool-execution-passthrough-mode)
- [API Examples](#-api-examples)
- [ACE Framework Working Nature](#-ace-framework-working-nature)
- [Roadmap](#-roadmap)
- [Links](#-links)

---

# ğŸ§© What Lynkr Solves

### **The Problem**
Claude Code is exceptionally usefulâ€”but it only communicates with Anthropicâ€™s hosted backend.

This means:

âŒ You canâ€™t point Claude Code at **Databricks LLMs**  
âŒ You canâ€™t run Claude workflows **locally**, offline, or in secure contexts  
âŒ MCP tools must be managed manually  
âŒ You donâ€™t control caching, policies, logs, or backend behavior

### **The Solution: Lynkr**
Lynkr is a **Claude Code-compatible backend** that sits between the CLI and your actual model provider.

```

Claude Code CLI
â†“
Lynkr Proxy
â†“
Databricks / Azure Anthropic / OpenRouter / Ollama / llama.cpp / MCP / Tools

```

This enables:

- **Databricks-native LLM development**
- **Enterprise-private model usage**
- **LLM agents with Git + file system access**
- **Smart workflows via MCP**
- **Transparent caching + logging**

---

# ğŸ— Architecture Overview

```

Claude Code CLI
â†“  (HTTP POST /v1/messages)
Lynkr Proxy (Node.js + Express)
â†“
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”‚  Orchestrator (Agent Loop)          â”‚
â”‚  â”œâ”€ Tool Execution Pipeline         â”‚
â”‚  â”œâ”€ Long-Term Memory System         â”‚
â”‚  â”œâ”€ MCP Registry + Sandbox          â”‚
â”‚  â”œâ”€ Prompt Cache (LRU + TTL)        â”‚
â”‚  â”œâ”€ Session Store (SQLite)          â”‚
â”‚  â”œâ”€ Repo Indexer (Tree-sitter)      â”‚
â”‚  â”œâ”€ Policy Engine                   â”‚
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â†“
Databricks / Azure Anthropic / OpenRouter / Ollama / llama.cpp

````

Key directories:

- `src/api` â†’ Claude-compatible API proxy
- `src/orchestrator` â†’ LLM agent runtime loop
- `src/memory` â†’ Long-term memory system (Titans-inspired)
- `src/mcp` â†’ Model Context Protocol tooling
- `src/tools` â†’ Git, diff, test, tasks, fs tools
- `src/cache` â†’ prompt caching backend
- `src/indexer` â†’ repo intelligence

---

# âš™ Installation

## Global install (recommended)
```bash
npm install -g lynkr
lynkr start
````

## Homebrew

```bash
brew tap vishalveerareddy123/lynkr
brew install vishalveerareddy123/lynkr/lynkr
```

## From source

```bash
git clone https://github.com/vishalveerareddy123/Lynkr.git
cd Lynkr
npm install
npm start
```

---

# ğŸ”§ Configuring Providers

## Databricks Setup

```env
MODEL_PROVIDER=databricks
DATABRICKS_API_BASE=https://<workspace>.cloud.databricks.com
DATABRICKS_API_KEY=<personal-access-token>
DATABRICKS_ENDPOINT_PATH=/serving-endpoints/databricks-claude-sonnet-4-5/invocations
WORKSPACE_ROOT=/path/to/your/repo
PORT=8080
```

## Azure Anthropic Setup

```env
MODEL_PROVIDER=azure-anthropic
AZURE_ANTHROPIC_ENDPOINT=https://<resource>.services.ai.azure.com/anthropic/v1/messages
AZURE_ANTHROPIC_API_KEY=<api-key>
AZURE_ANTHROPIC_VERSION=2023-06-01
WORKSPACE_ROOT=/path/to/repo
PORT=8080
```

## Azure OpenAI Setup

```env
MODEL_PROVIDER=azure-openai
AZURE_OPENAI_ENDPOINT=https://<resource>.openai.azure.com
AZURE_OPENAI_API_KEY=<api-key>
AZURE_OPENAI_DEPLOYMENT=gpt-4o
PORT=8080
```


## OpenRouter Setup

**What is OpenRouter?**

OpenRouter provides unified access to 100+ AI models (GPT-4o, Claude, Gemini, Llama, etc.) through a single API. Benefits:
- âœ… No vendor lock-in - switch models without code changes
- âœ… Competitive pricing ($0.15/$0.60 per 1M for GPT-4o-mini)
- âœ… Automatic fallbacks if primary model unavailable
- âœ… Pay-as-you-go, no monthly fees
- âœ… Full tool calling support

**Configuration:**

```env
MODEL_PROVIDER=openrouter
OPENROUTER_API_KEY=sk-or-v1-...                                    # Get from https://openrouter.ai/keys
OPENROUTER_MODEL=openai/gpt-4o-mini                                # See https://openrouter.ai/models
OPENROUTER_ENDPOINT=https://openrouter.ai/api/v1/chat/completions
PORT=8080
WORKSPACE_ROOT=/path/to/your/repo
```

**Popular Models:**
- `openai/gpt-4o-mini` â€“ Fast, affordable ($0.15/$0.60 per 1M)
- `anthropic/claude-3.5-sonnet` â€“ Claude's best reasoning
- `google/gemini-pro-1.5` â€“ Large context window
- `meta-llama/llama-3.1-70b-instruct` â€“ Open-source Llama

See [https://openrouter.ai/models](https://openrouter.ai/models) for complete list.

**Getting Started:**
1. Visit [https://openrouter.ai](https://openrouter.ai)
2. Sign in with GitHub/Google/email
3. Create API key at [https://openrouter.ai/keys](https://openrouter.ai/keys)
4. Add credits (minimum $5)
5. Configure Lynkr as shown above

## llama.cpp Setup

**What is llama.cpp?**

llama.cpp is a high-performance C++ inference engine for running GGUF models locally. Benefits:
- âœ… **Maximum performance** â€“ Optimized C++ inference
- âœ… **Any GGUF model** â€“ Run any model from HuggingFace
- âœ… **Lower memory usage** â€“ Advanced quantization options (Q2_K to Q8_0)
- âœ… **Multi-GPU support** â€“ CUDA, Metal, ROCm, Vulkan
- âœ… **OpenAI-compatible API** â€“ Seamless integration
- âœ… **Full tool calling** â€“ Grammar-based, reliable

**Configuration:**

```env
MODEL_PROVIDER=llamacpp
LLAMACPP_ENDPOINT=http://localhost:8080    # llama-server default port
LLAMACPP_MODEL=qwen2.5-coder-7b            # Model name (for logging)
LLAMACPP_TIMEOUT_MS=120000                 # Request timeout
PORT=8080
WORKSPACE_ROOT=/path/to/your/repo
```

**Setup Steps:**

```bash
# 1. Build llama.cpp (or download pre-built binary)
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp && make

# 2. Download a GGUF model (example: Qwen2.5-Coder)
wget https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_k_m.gguf

# 3. Start llama-server
./llama-server -m qwen2.5-coder-7b-instruct-q4_k_m.gguf --port 8080

# 4. Verify server is running
curl http://localhost:8080/health
```

**llama.cpp vs Ollama:**

| Feature | Ollama | llama.cpp |
|---------|--------|-----------|
| Setup | Easy (app) | Manual (compile/download) |
| Model Format | Ollama-specific | Any GGUF model |
| Performance | Good | Excellent |
| Memory Usage | Higher | Lower (quantization) |
| API | Custom | OpenAI-compatible |
| Flexibility | Limited models | Any GGUF from HuggingFace |

Choose llama.cpp when you need maximum performance, specific quantization options, or GGUF models not available in Ollama.

---

# ğŸ’¬ Using Lynkr With Claude Code CLI

```bash
export ANTHROPIC_BASE_URL=http://localhost:8080
export ANTHROPIC_API_KEY=dummy
```

Then:

```bash
claude chat
claude diff
claude review
claude apply
```

Everything routes through your configured model provider (Databricks, Azure, OpenRouter, Ollama, or llama.cpp).

---

# ğŸ§  Repo Intelligence & Indexing

Lynkr uses Tree-sitter and SQLite to analyze your workspace:

* **Symbol definitions**
* **Cross-file references**
* **Language mix**
* **Framework hints**
* **Dependency patterns**
* **Testing metadata**

It generates a structured `CLAUDE.md` so the model always has context.

---

# ğŸ§  Long-Term Memory System (Titans-Inspired)

Lynkr includes a sophisticated long-term memory system inspired by Google's Titans architecture, enabling persistent learning across conversations without model retraining.

## How It Works

The memory system automatically:

1. **Extracts** important information from conversations (preferences, decisions, facts, entities, relationships)
2. **Filters** using surprise-based scoring to store only novel/important information
3. **Retrieves** relevant memories using multi-signal ranking (recency + importance + relevance)
4. **Injects** top memories into each request for contextual continuity

## Key Features

### ğŸ¯ Surprise-Based Memory Updates (Titans Core Innovation)

Memories are scored 0.0-1.0 based on five factors:
- **Novelty** (30%): New entities/concepts not seen before
- **Contradiction** (40%): Conflicts with existing memories
- **Specificity** (15%): Level of detail and technical depth
- **User Emphasis** (10%): Explicit emphasis markers (IMPORTANT, CRITICAL, etc.)
- **Context Switch** (5%): Topic changes

Only memories exceeding the surprise threshold (default 0.3) are stored, preventing redundancy.

### ğŸ” FTS5 Semantic Search

Uses SQLite's full-text search with Porter stemming for keyword-based semantic search:
- No external dependencies or embedding models
- Sub-millisecond search performance
- Supports Boolean operators (AND, OR, phrase search)

### ğŸ“Š Multi-Signal Retrieval

Ranks memories using weighted combination:
- **Recency** (30%): Recent memories weighted higher with 7-day exponential decay
- **Importance** (40%): Stored importance score (preference=0.7, decision=0.8, fact=0.6)
- **Relevance** (30%): Keyword overlap with current query

### ğŸ—‚ï¸ Memory Types

- **Preferences**: User coding styles, tool choices, frameworks
- **Decisions**: Architectural choices, agreed approaches
- **Facts**: Project details, tech stack, configurations
- **Entities**: Classes, functions, files, libraries mentioned
- **Relationships**: Dependencies, imports, extends patterns

## Configuration

All features are enabled by default with sensible defaults:

```env
# Core Settings
MEMORY_ENABLED=true                    # Master switch
MEMORY_RETRIEVAL_LIMIT=5               # Memories per request
MEMORY_SURPRISE_THRESHOLD=0.3          # Novelty filter (0.0-1.0)

# Lifecycle Management
MEMORY_MAX_AGE_DAYS=90                 # Auto-delete old memories
MEMORY_MAX_COUNT=10000                 # Maximum total memories
MEMORY_DECAY_ENABLED=true              # Enable importance decay
MEMORY_DECAY_HALF_LIFE=30              # Days for 50% importance decay

# Retrieval Behavior
MEMORY_INCLUDE_GLOBAL=true             # Include cross-session memories
MEMORY_INJECTION_FORMAT=system         # Where to inject (system/assistant_preamble)
MEMORY_EXTRACTION_ENABLED=true         # Auto-extract from responses
```

## Performance

Exceeds all targets:
- **Retrieval**: <2ms average (50x faster than 50ms target)
- **Extraction**: <3ms average (40x faster than 100ms target)
- **Storage**: ~150 bytes per memory
- **Search**: Sub-millisecond FTS5 queries
- **Surprise Calculation**: <1ms average

## Example Usage

The system works automatically - no manual intervention needed:

```bash
# First conversation
User: "I prefer Python for data processing"
Assistant: "I'll remember that you prefer Python..."
# System extracts: [preference] "prefer Python for data processing" (surprise: 0.85)

# Later conversation (same or different session)
User: "Write a script to process this CSV"
# System retrieves: [preference] "prefer Python for data processing"
Assistant: "I'll write a Python script using pandas..."
```

## Database Tables

- **`memories`**: Core memory storage (content, type, importance, surprise_score)
- **`memories_fts`**: FTS5 full-text search index (auto-synced via triggers)
- **`memory_entities`**: Entity tracking for novelty detection
- **`memory_embeddings`**: Optional vector storage (Phase 3, not yet used)
- **`memory_associations`**: Memory graph relationships (Phase 5, not yet used)

## Memory Tools (Optional)

Explicit memory management tools available:
- `memory_search` - Search long-term memories by query
- `memory_add` - Manually add important facts
- `memory_forget` - Remove memories matching query
- `memory_stats` - View memory statistics

Enable by exposing tools to the model (configurable in orchestrator).

---

# âš¡ Prompt Caching

Lynkr includes an LRU+TTL prompt cache.

### Benefits:

* Reduced Databricks compute consumption
* Faster response times
* Deterministic repeated responses

Configure:

```env
PROMPT_CACHE_ENABLED=true
PROMPT_CACHE_TTL_MS=300000
PROMPT_CACHE_MAX_ENTRIES=64
```

---

# ğŸ§© Model Context Protocol (MCP)

Lynkr automatically discovers MCP manifests from:

```
~/.claude/mcp
```

or directories defined via:

```
MCP_MANIFEST_DIRS
```

MCP tools become available inside the Claude Code environment, including:

* GitHub integrations
* Jira automations
* custom internal tools
* filesystem operations
* build systems
* CI/CD triggers

Optional sandboxing uses Docker or OCI runtimes.

---

# ğŸ”§ Git Tools

Lynkr includes a full suite of Git operations:

* `workspace_git_status`
* `workspace_git_diff`
* `workspace_git_stage`
* `workspace_git_commit`
* `workspace_git_push`
* `workspace_git_pull`
* Release-note generation
* Diff summarization & analysis

Policies:

* `POLICY_GIT_ALLOW_PUSH`
* `POLICY_GIT_REQUIRE_TESTS`
* `POLICY_GIT_TEST_COMMAND`

Example:

> Disallow push unless tests pass?
> Set `POLICY_GIT_REQUIRE_TESTS=true`.

---

# ğŸ”„ Client-Side Tool Execution (Passthrough Mode)

Lynkr supports **client-side tool execution**, enabling tools to execute on the Claude Code CLI machine instead of the proxy server.

**Enable passthrough mode:**

```bash
export TOOL_EXECUTION_MODE=client
npm start
```

**How it works:**

1. Model generates tool calls (from Databricks/OpenRouter/Ollama/llama.cpp)
2. Proxy converts to Anthropic format with `tool_use` blocks
3. Claude Code CLI receives `tool_use` blocks and executes locally
4. CLI sends `tool_result` blocks back in the next request
5. Proxy forwards complete conversation back to the model

**Benefits:**

* âœ… Local filesystem access on CLI user's machine
* âœ… Local credentials, SSH keys, environment variables
* âœ… Integration with local dev tools (git, npm, docker)
* âœ… Reduced network latency for file operations
* âœ… Server doesn't need filesystem permissions

**Use cases:**

* Remote proxy server with local CLI execution
* Multi-user environments where each needs their own workspace
* Security-sensitive setups where server shouldn't access user files

**Configuration:**

* `TOOL_EXECUTION_MODE=server` â€“ Tools run on proxy (default)
* `TOOL_EXECUTION_MODE=client` â€“ Tools run on CLI side
* `TOOL_EXECUTION_MODE=passthrough` â€“ Alias for client mode

---

# ğŸ§ª API Example (Index Rebuild)

```bash
curl http://localhost:8080/v1/messages \
  -H 'Content-Type: application/json' \
  -d '{
    "model": "claude-proxy",
    "messages": [{ "role": "user", "content": "Rebuild the index." }],
    "tool_choice": {
      "type": "function",
      "function": { "name": "workspace_index_rebuild" }
    }
  }'
```

---


# ğŸ¤– ACE Framework Working Nature

Lynkr's agentic architecture is inspired by the **Autonomous Cognitive Entity (ACE) Framework**, specifically implementing the **Reflector** pattern to enable self-improving capabilities.

### The Agentic Loop

1.  **Input Processing**: The **Orchestrator** receives natural language intent from the user.
2.  **Execution (Agent Model)**: The system executes tools (Git, Search, File Ops) to achieve the goal.
3.  **Reflection (Reflector Role)**: After execution types, the `Reflector` agent analyzes the transcript to extract "skills" and optimize future performance.

### The Reflector
The Reflector (`src/agents/reflector.js`) is an introspective component that analyzes:
*   **Tool Usage Patterns**: Identifying effective tool combinations (e.g., "Search -> Read -> Fix").
*   **Efficiency**: Calculating step-count and token efficiency.
*   **Error Handling**: Learning from recovered errors to suggest robust fallback strategies.
*   **Task Patterns**: Recognizing recurring task types (Refactoring, Testing, Documentation) and their optimal workflows.

This "working nature" allows Lynkr to not just execute commands, but to **learn from interaction**, continuously refining its internal heuristics for tool selection and planning.

---

# ğŸ›£ Roadmap

## âœ… Recently Completed (December 2025)

* **llama.cpp Provider Support** â€“ Run any GGUF model locally with maximum performance using llama.cpp's optimized C++ inference engine with full tool calling support
* **Titans-Inspired Long-Term Memory System** â€“ Automatic extraction and retrieval of conversation memories using surprise-based filtering, FTS5 semantic search, and multi-signal ranking for persistent learning across sessions
* **Client-side tool execution** (`TOOL_EXECUTION_MODE=client/passthrough`) â€“ Tools can execute on the Claude Code CLI side, enabling local file operations, commands, and access to local credentials
* **OpenRouter error resilience** â€“ Graceful handling of malformed OpenRouter responses, preventing crashes during rate limits or service errors
* **Enhanced format conversion** â€“ Improved Anthropic â†” OpenRouter format conversion for tool calls with proper `tool_use` block generation

## ğŸ”® Future Features

* **Memory System Enhancements**:
  * Local embeddings with ONNX runtime for true semantic search (Phase 3)
  * Memory association graphs for relationship-based retrieval (Phase 5)
  * Memory decay scheduler with background optimization
* LSP integration (TypeScript, Python, more languages)
* Per-file diff comments
* Risk scoring for Git diffs
* Expand MCP support
* Skill-like declarative automation layer
* Historical test dashboards
* Databricks-specific tools

---

# ğŸ”— Links

* **GitHub**: [https://github.com/vishalveerareddy123/Lynkr](https://github.com/vishalveerareddy123/Lynkr)
* **Docs**: [https://deepwiki.com/vishalveerareddy123/Lynkr](https://deepwiki.com/vishalveerareddy123/Lynkr)
* **Issues**: [https://github.com/vishalveerareddy123/Lynkr/issues](https://github.com/vishalveerareddy123/Lynkr/issues)

If you use Databricks or Azure Anthropic and want rich Claude Code workflows, Lynkr gives you the control and extensibility you need.

Feel free to open issues, contribute tools, or integrate with MCP servers!

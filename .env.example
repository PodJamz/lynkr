# Lynkr + Ollama Configuration Example
# Copy this file to .env and fill in your values

# ==============================================================================
# Model Provider Configuration
# ==============================================================================

# Primary model provider to use
# Options: databricks, azure-anthropic, azure-openai, openrouter, openai, ollama, llamacpp
# Default: databricks
# MODEL_PROVIDER=databricks

# ==============================================================================
# Ollama Configuration (Hybrid Routing)
# ==============================================================================

# Enable Ollama preference for simple requests
PREFER_OLLAMA=true

# Ollama model to use (must be compatible with tool calling)
# Options: qwen2.5-coder:latest, llama3.1, mistral-nemo, etc.
OLLAMA_MODEL=qwen2.5-coder:latest

# Fallback provider when primary provider fails or for complex requests
# Options: databricks, azure-anthropic, azure-openai, openrouter, openai, llamacpp
FALLBACK_PROVIDER=databricks

# Enable automatic fallback (true = transparent fallback, false = fail on provider error)
FALLBACK_ENABLED=true

# Max tools for routing to Ollama (requests with more tools go to cloud)
OLLAMA_MAX_TOOLS_FOR_ROUTING=3

# ==============================================================================
# Databricks Configuration (Required for Fallback)
# ==============================================================================

# DATABRICKS_API_BASE=https://your-workspace.cloud.databricks.com
# DATABRICKS_API_KEY=dapi1234567890abcdef

# ==============================================================================
# Azure Anthropic Configuration (Optional Alternative Fallback)
# ==============================================================================

# AZURE_ANTHROPIC_ENDPOINT=https://your-anthropic.openai.azure.com
# AZURE_ANTHROPIC_API_KEY=your-azure-key

# ==============================================================================
# Azure OpenAI Configuration (Optional Alternative Fallback)
# ==============================================================================

# Azure OpenAI resource endpoint (from Azure Portal)
# Format: https://<your-resource-name>.openai.azure.com
# AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com

# Azure OpenAI API key (found in Azure Portal under Keys and Endpoint)
# AZURE_OPENAI_API_KEY=your-azure-openai-key

# Deployment name (the model deployment you created in Azure)
# This can point to any Azure OpenAI model: gpt-4o, gpt-5, gpt-5-codex, etc.
# Default: gpt-4o
# AZURE_OPENAI_DEPLOYMENT=gpt-4o

# API version (use latest stable version for best features)
# Default: 2024-08-01-preview
# AZURE_OPENAI_API_VERSION=2024-08-01-preview

# ==============================================================================
# OpenAI Configuration (Direct OpenAI API)
# ==============================================================================

# OpenAI API key (from https://platform.openai.com/api-keys)
# OPENAI_API_KEY=sk-your-openai-api-key

# OpenAI model to use
# Options: gpt-4o, gpt-4o-mini, gpt-4-turbo, gpt-3.5-turbo, o1-preview, o1-mini
# Default: gpt-4o
# OPENAI_MODEL=gpt-4o

# OpenAI API endpoint (usually don't need to change this)
# Default: https://api.openai.com/v1/chat/completions
# OPENAI_ENDPOINT=https://api.openai.com/v1/chat/completions

# OpenAI organization ID (optional, for organization-level API keys)
# OPENAI_ORGANIZATION=org-your-org-id

# ==============================================================================
# llama.cpp Configuration (Local GGUF Models)
# ==============================================================================

# llama.cpp server endpoint (default: http://localhost:8080)
# Start with: ./llama-server -m model.gguf --port 8080
# LLAMACPP_ENDPOINT=http://localhost:8080

# Model name (for logging purposes, llama.cpp uses the loaded model)
# LLAMACPP_MODEL=default

# Request timeout in milliseconds (default: 120000)
# LLAMACPP_TIMEOUT_MS=120000

# Optional API key (for secured llama.cpp servers)
# LLAMACPP_API_KEY=your-optional-api-key

# ==============================================================================
# Server Configuration
# ==============================================================================

PORT=8080
LOG_LEVEL=info
WEB_SEARCH_ENDPOINT=http://localhost:8888/search

# Tool execution mode: where to execute tools (Write, Read, Bash, etc.)
# - server: Execute tools on the server (default, for standalone proxy use)
# - passthrough: Return tool calls to CLI for local execution (for Claude Code CLI compatibility)
# TOOL_EXECUTION_MODE=server

# ==============================================================================
# Subagent Configuration (Server Mode - matches Claude Code architecture)
# ==============================================================================

# Enable server-side subagents
# When enabled, the main agent can spawn specialized subagents for complex tasks
# AGENTS_ENABLED=true

# Max concurrent subagents (Claude Code uses up to 10, batched)
# AGENTS_MAX_CONCURRENT=10

# Default model for subagents
# Options: haiku (fast/cheap), sonnet (smart), gpt-4o-mini, gpt-4o
# AGENTS_DEFAULT_MODEL=haiku

# Max steps per subagent
# AGENTS_MAX_STEPS=15

# Timeout per subagent (milliseconds)
# AGENTS_TIMEOUT=120000

# ==============================================================================
# Long-Term Memory System (Titans-Inspired)
# ==============================================================================

# Enable/disable the entire memory system
# When enabled, automatically extracts and retrieves conversation memories
# Default: true
# MEMORY_ENABLED=true

# Maximum number of memories to inject into each request
# Higher = more context but larger prompts
# Default: 5, Range: 1-20
# MEMORY_RETRIEVAL_LIMIT=5

# Minimum surprise score (0.0-1.0) required to store a memory
# Filters out redundant information
# Lower (0.1-0.2) = store more memories
# Higher (0.4-0.5) = only store highly novel information
# Default: 0.3
# MEMORY_SURPRISE_THRESHOLD=0.3

# Auto-delete memories older than this many days
# Prevents database bloat with stale information
# Default: 90
# MEMORY_MAX_AGE_DAYS=90

# Maximum total memories to keep (prunes least important when exceeded)
# Default: 10000
# MEMORY_MAX_COUNT=10000

# Enable importance decay over time (exponential decay)
# Memories become less important as they age
# Default: true
# MEMORY_DECAY_ENABLED=true

# Days for importance to decay by 50% (exponential half-life)
# Shorter (7-14) = prefer recent info
# Longer (60-90) = value historical context
# Default: 30
# MEMORY_DECAY_HALF_LIFE=30

# Include memories with session_id=NULL (global memories) in all sessions
# Global memories = project facts shared across all conversations
# Default: true
# MEMORY_INCLUDE_GLOBAL=true

# Where to inject memories in the prompt
# Options: system (recommended), assistant_preamble
# Default: system
# MEMORY_INJECTION_FORMAT=system

# Enable automatic extraction of memories from assistant responses
# Disable for read-only memory mode
# Default: true
# MEMORY_EXTRACTION_ENABLED=true

# ==============================================================================
# Performance & Security
# ==============================================================================

# API retry configuration
# API_RETRY_MAX_RETRIES=3
# API_RETRY_INITIAL_DELAY=1000
# API_RETRY_MAX_DELAY=30000

# Load shedding thresholds
# LOAD_SHEDDING_HEAP_THRESHOLD=90
# LOAD_SHEDDING_EVENT_LOOP_DELAY=100

# ==============================================================================
# Notes
# ==============================================================================

# For Docker Compose:
# 1. Copy this file to .env
# 2. Fill in your Databricks credentials
# 3. Run: docker-compose up -d
# 4. Wait for Ollama to download the model (first run takes time)
# 5. Access Lynkr at http://localhost:8080

# For standalone Ollama (no Docker):
# 1. Install Ollama: https://ollama.ai/download
# 2. Pull model: ollama pull qwen2.5-coder:latest
# 3. Set OLLAMA_ENDPOINT to http://localhost:11434
# 4. Run Lynkr: npm start

# Lynkr + Ollama Configuration Example
# Copy this file to .env and fill in your values

# ==============================================================================
# Ollama Configuration (Hybrid Routing)
# ==============================================================================

# Enable Ollama preference for simple requests
PREFER_OLLAMA=true

# Ollama model to use (must be compatible with tool calling)
# Options: qwen2.5-coder:latest, llama3.1, mistral-nemo, etc.
OLLAMA_MODEL=qwen2.5-coder:latest

# Fallback provider when Ollama fails or for complex requests
# Options: databricks, azure-anthropic
OLLAMA_FALLBACK_PROVIDER=databricks

# Enable automatic fallback (true = transparent fallback, false = fail on Ollama error)
OLLAMA_FALLBACK_ENABLED=true

# Max tools for routing to Ollama (requests with more tools go to cloud)
OLLAMA_MAX_TOOLS_FOR_ROUTING=3

# ==============================================================================
# Databricks Configuration (Required for Fallback)
# ==============================================================================

# DATABRICKS_API_BASE=https://your-workspace.cloud.databricks.com
# DATABRICKS_API_KEY=dapi1234567890abcdef

# ==============================================================================
# Azure Anthropic Configuration (Optional Alternative Fallback)
# ==============================================================================

# AZURE_ANTHROPIC_ENDPOINT=https://your-anthropic.openai.azure.com
# AZURE_ANTHROPIC_API_KEY=your-azure-key

# ==============================================================================
# Server Configuration
# ==============================================================================

PORT=8080
LOG_LEVEL=info
WEB_SEARCH_ENDPOINT=http://localhost:8888/search

# ==============================================================================
# Performance & Security
# ==============================================================================

# API retry configuration
# API_RETRY_MAX_RETRIES=3
# API_RETRY_INITIAL_DELAY=1000
# API_RETRY_MAX_DELAY=30000

# Load shedding thresholds
# LOAD_SHEDDING_HEAP_THRESHOLD=90
# LOAD_SHEDDING_EVENT_LOOP_DELAY=100

# ==============================================================================
# Notes
# ==============================================================================

# For Docker Compose:
# 1. Copy this file to .env
# 2. Fill in your Databricks credentials
# 3. Run: docker-compose up -d
# 4. Wait for Ollama to download the model (first run takes time)
# 5. Access Lynkr at http://localhost:8080

# For standalone Ollama (no Docker):
# 1. Install Ollama: https://ollama.ai/download
# 2. Pull model: ollama pull qwen2.5-coder:latest
# 3. Set OLLAMA_ENDPOINT to http://localhost:11434
# 4. Run Lynkr: npm start

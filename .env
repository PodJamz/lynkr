
# Primary model provider
# Options: ollama, openrouter, azure-openai, openai, azure-anthropic, databricks, llamacpp
MODEL_PROVIDER=llamacpp

# Ollama endpoint (local by default)
OLLAMA_ENDPOINT=http://localhost:11434

# Ollama model to use (must support tool calling for best results)
# Recommended: llama3.1:8b, llama3.2, qwen3, mistral:7b-instruct
# NOT recommended for tools: qwen2.5-coder (code-only, slow with tools)
OLLAMA_MODEL=llama3.1:8b

# Disable fallback to cloud providers (Ollama-only mode)
FALLBACK_ENABLED=false

# Note: PREFER_OLLAMA is ignored when MODEL_PROVIDER=ollama
# Max tools before performance degradation (auto-limited to 8 essential tools)
OLLAMA_MAX_TOOLS_FOR_ROUTING=2

# ==============================================================================
# OpenRouter Configuration (Multi-Model Provider)
# ==============================================================================

# OpenRouter API key (get from https://openrouter.ai/keys)
OPENROUTER_API_KEY=<your-openrouter-api-key>


# OpenRouter model to use
# Options: openai/gpt-4o, anthropic/claude-3.5-sonnet, google/gemini-pro-1.5, openai/gpt-oss-120b, etc.
# gpt-4o-mini is recommended for affordable tool calling ($0.15/$0.60 per 1M tokens)
OPENROUTER_MODEL=amazon/nova-2-lite-v1:free

# OpenRouter endpoint
OPENROUTER_ENDPOINT=https://openrouter.ai/api/v1/chat/completions

# Max tools before routing to heavier provider (OpenRouter handles more than Ollama)
OPENROUTER_MAX_TOOLS_FOR_ROUTING=15

# ==============================================================================
# Cloud Provider Credentials (NOT NEEDED for Ollama-only mode)
# ==============================================================================
# These are commented out since we're running in Ollama-only mode.
# Uncomment and configure if you want to enable fallback.

# Databricks (optional fallback)
#DATABRICKS_API_BASE=https://a1db-7405615420308831.11.azuredatabricks.net
#DATABRICKS_API_KEY=dapi51a744e900a5b5cd993206929ff222b73

# Azure Anthropic (optional fallback)
# AZURE_ANTHROPIC_ENDPOINT=https://your-endpoint.openai.azure.com
# AZURE_ANTHROPIC_API_KEY=your-azure-key

# Azure OpenAI (optional fallback)
# Azure OpenAI resource endpoint (from Azure Portal)
# Format: https://<your-resource-name>.openai.azure.com
AZURE_OPENAI_ENDPOINT=https://your-endpoint.openai.azure.com
# Azure OpenAI API key (found in Azure Portal under Keys and Endpoint)
AZURE_OPENAI_API_KEY=XXX-YYYYY

# Deployment name (the model deployment you created in Azure)
# This can point to any Azure model: gpt-4o, gpt-5, gpt-5-codex, etc.
AZURE_OPENAI_DEPLOYMENT=Kimi-K2-Thinking

# ==============================================================================
# OpenAI Configuration (Direct OpenAI API)
# ==============================================================================

# OpenAI API key (from https://platform.openai.com/api-keys)
# OPENAI_API_KEY=sk-your-openai-api-key

# OpenAI model to use
# Options: gpt-4o, gpt-4o-mini, gpt-4-turbo, gpt-3.5-turbo, o1-preview, o1-mini
# Default: gpt-4o
# OPENAI_MODEL=gpt-4o

# OpenAI API endpoint (usually don't need to change this)
# Default: https://api.openai.com/v1/chat/completions
# OPENAI_ENDPOINT=https://api.openai.com/v1/chat/completions

# OpenAI organization ID (optional, for organization-level API keys)
# OPENAI_ORGANIZATION=org-your-org-id

# ==============================================================================
# llama.cpp Configuration (Local GGUF Models)
# ==============================================================================

# llama.cpp server endpoint (default: http://localhost:8080)
# Start with: ./llama-server -m model.gguf --port 8080
LLAMACPP_ENDPOINT=http://localhost:8080

# Model name (for logging purposes, llama.cpp uses the loaded model)
LLAMACPP_MODEL=default

# Request timeout in milliseconds (default: 120000)
LLAMACPP_TIMEOUT_MS=120000

# Optional API key (for secured llama.cpp servers)
# LLAMACPP_API_KEY=

# ==============================================================================
# Server Configuration
# ==============================================================================

PORT=8081
LOG_LEVEL=debug  # Use 'debug' for detailed logs, 'info' for production
WEB_SEARCH_ENDPOINT=http://localhost:8888/search

# Tool execution mode: where to execute tools (Write, Read, Bash, etc.)
# - server: Execute tools on the server (default)
# - passthrough/client: Return tool calls to CLI for local execution
TOOL_EXECUTION_MODE=client

# ==============================================================================
# Long-Term Memory System (Titans-Inspired)
# ==============================================================================

# Enable/disable the entire memory system
# When enabled, automatically extracts and retrieves conversation memories
# Default: true
MEMORY_ENABLED=true

# Maximum number of memories to inject into each request
# Higher = more context but larger prompts
# Default: 5, Range: 1-20
MEMORY_RETRIEVAL_LIMIT=5

# Minimum surprise score (0.0-1.0) required to store a memory
# Filters out redundant information
# Lower (0.1-0.2) = store more memories
# Higher (0.4-0.5) = only store highly novel information
# Default: 0.3
MEMORY_SURPRISE_THRESHOLD=0.3

# Auto-delete memories older than this many days
# Prevents database bloat with stale information
# Default: 90
MEMORY_MAX_AGE_DAYS=90

# Maximum total memories to keep (prunes least important when exceeded)
# Default: 10000
MEMORY_MAX_COUNT=10000

# Enable importance decay over time (exponential decay)
# Memories become less important as they age
# Default: true
MEMORY_DECAY_ENABLED=true

# Days for importance to decay by 50% (exponential half-life)
# Shorter (7-14) = prefer recent info
# Longer (60-90) = value historical context
# Default: 30
MEMORY_DECAY_HALF_LIFE=30

# Include memories with session_id=NULL (global memories) in all sessions
# Global memories = project facts shared across all conversations
# Default: true
MEMORY_INCLUDE_GLOBAL=true

# Where to inject memories in the prompt
# Options: system (recommended), assistant_preamble
# Default: system
MEMORY_INJECTION_FORMAT=system

# Enable automatic extraction of memories from assistant responses
# Disable for read-only memory mode
# Default: true
MEMORY_EXTRACTION_ENABLED=true

# ==============================================================================
# Performance Tuning (Optional)
# ==============================================================================

# Load shedding thresholds
LOAD_SHEDDING_HEAP_THRESHOLD=0.95
# LOAD_SHEDDING_EVENT_LOOP_DELAY=100
